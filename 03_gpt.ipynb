{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT2 Large Language Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpt_2_simple as gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching checkpoint: 1.05Mit [00:00, 3.54Git/s]                                                     \n",
      "Fetching encoder.json: 1.05Mit [00:03, 345kit/s]                                                    \n",
      "Fetching hparams.json: 1.05Mit [00:00, 11.3Git/s]                                                   \n",
      "Fetching model.ckpt.data-00000-of-00001: 498Mit [22:04, 376kit/s]                                   \n",
      "Fetching model.ckpt.index: 1.05Mit [00:00, 542Mit/s]                                                \n",
      "Fetching model.ckpt.meta: 1.05Mit [00:00, 1.19Mit/s]                                                \n",
      "Fetching vocab.bpe: 1.05Mit [00:01, 955kit/s]                                                       \n"
     ]
    }
   ],
   "source": [
    "# gpt2.download_gpt2(\n",
    "     #model_name='124M'\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Variable model/wpe does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=tf.AUTO_REUSE in VarScope?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m session \u001b[39m=\u001b[39m gpt2\u001b[39m.\u001b[39mstart_tf_sess()\n\u001b[0;32m----> 2\u001b[0m gpt2\u001b[39m.\u001b[39;49mload_gpt2(session, model_name\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m124M\u001b[39;49m\u001b[39m'\u001b[39;49m, reuse\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/gpt_2_simple/gpt_2.py:394\u001b[0m, in \u001b[0;36mload_gpt2\u001b[0;34m(sess, checkpoint, run_name, checkpoint_dir, model_name, model_dir, multi_gpu, reuse)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[39mif\u001b[39;00m multi_gpu:\n\u001b[1;32m    392\u001b[0m     gpus \u001b[39m=\u001b[39m get_available_gpus()\n\u001b[0;32m--> 394\u001b[0m output \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mmodel(hparams\u001b[39m=\u001b[39;49mhparams, X\u001b[39m=\u001b[39;49mcontext, gpus\u001b[39m=\u001b[39;49mgpus, reuse\u001b[39m=\u001b[39;49mreuse)\n\u001b[1;32m    396\u001b[0m \u001b[39mif\u001b[39;00m checkpoint\u001b[39m==\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlatest\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    397\u001b[0m     ckpt \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mtrain\u001b[39m.\u001b[39mlatest_checkpoint(checkpoint_path)\n",
      "File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/gpt_2_simple/src/model.py:188\u001b[0m, in \u001b[0;36mmodel\u001b[0;34m(hparams, X, past, scope, gpus, reuse)\u001b[0m\n\u001b[1;32m    185\u001b[0m results \u001b[39m=\u001b[39m {}\n\u001b[1;32m    186\u001b[0m batch, sequence \u001b[39m=\u001b[39m shape_list(X)\n\u001b[0;32m--> 188\u001b[0m wpe \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mcompat\u001b[39m.\u001b[39;49mv1\u001b[39m.\u001b[39;49mget_variable(\u001b[39m'\u001b[39;49m\u001b[39mwpe\u001b[39;49m\u001b[39m'\u001b[39;49m, [hparams\u001b[39m.\u001b[39;49mn_ctx, hparams\u001b[39m.\u001b[39;49mn_embd],\n\u001b[1;32m    189\u001b[0m                      initializer\u001b[39m=\u001b[39;49mtf\u001b[39m.\u001b[39;49mcompat\u001b[39m.\u001b[39;49mv1\u001b[39m.\u001b[39;49mrandom_normal_initializer(stddev\u001b[39m=\u001b[39;49m\u001b[39m0.01\u001b[39;49m))\n\u001b[1;32m    190\u001b[0m wte \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mcompat\u001b[39m.\u001b[39mv1\u001b[39m.\u001b[39mget_variable(\u001b[39m'\u001b[39m\u001b[39mwte\u001b[39m\u001b[39m'\u001b[39m, [hparams\u001b[39m.\u001b[39mn_vocab, hparams\u001b[39m.\u001b[39mn_embd],\n\u001b[1;32m    191\u001b[0m                      initializer\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mcompat\u001b[39m.\u001b[39mv1\u001b[39m.\u001b[39mrandom_normal_initializer(stddev\u001b[39m=\u001b[39m\u001b[39m0.02\u001b[39m))\n\u001b[1;32m    192\u001b[0m past_length \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mif\u001b[39;00m past \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m tf\u001b[39m.\u001b[39mshape(\u001b[39minput\u001b[39m\u001b[39m=\u001b[39mpast)[\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m]\n",
      "File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/tensorflow/python/ops/variable_scope.py:1565\u001b[0m, in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m   1549\u001b[0m \u001b[39m@tf_export\u001b[39m(v1\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mget_variable\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m   1550\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_variable\u001b[39m(name,\n\u001b[1;32m   1551\u001b[0m                  shape\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1563\u001b[0m                  synchronization\u001b[39m=\u001b[39mVariableSynchronization\u001b[39m.\u001b[39mAUTO,\n\u001b[1;32m   1564\u001b[0m                  aggregation\u001b[39m=\u001b[39mVariableAggregation\u001b[39m.\u001b[39mNONE):\n\u001b[0;32m-> 1565\u001b[0m   \u001b[39mreturn\u001b[39;00m get_variable_scope()\u001b[39m.\u001b[39;49mget_variable(\n\u001b[1;32m   1566\u001b[0m       _get_default_variable_store(),\n\u001b[1;32m   1567\u001b[0m       name,\n\u001b[1;32m   1568\u001b[0m       shape\u001b[39m=\u001b[39;49mshape,\n\u001b[1;32m   1569\u001b[0m       dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m   1570\u001b[0m       initializer\u001b[39m=\u001b[39;49minitializer,\n\u001b[1;32m   1571\u001b[0m       regularizer\u001b[39m=\u001b[39;49mregularizer,\n\u001b[1;32m   1572\u001b[0m       trainable\u001b[39m=\u001b[39;49mtrainable,\n\u001b[1;32m   1573\u001b[0m       collections\u001b[39m=\u001b[39;49mcollections,\n\u001b[1;32m   1574\u001b[0m       caching_device\u001b[39m=\u001b[39;49mcaching_device,\n\u001b[1;32m   1575\u001b[0m       partitioner\u001b[39m=\u001b[39;49mpartitioner,\n\u001b[1;32m   1576\u001b[0m       validate_shape\u001b[39m=\u001b[39;49mvalidate_shape,\n\u001b[1;32m   1577\u001b[0m       use_resource\u001b[39m=\u001b[39;49muse_resource,\n\u001b[1;32m   1578\u001b[0m       custom_getter\u001b[39m=\u001b[39;49mcustom_getter,\n\u001b[1;32m   1579\u001b[0m       constraint\u001b[39m=\u001b[39;49mconstraint,\n\u001b[1;32m   1580\u001b[0m       synchronization\u001b[39m=\u001b[39;49msynchronization,\n\u001b[1;32m   1581\u001b[0m       aggregation\u001b[39m=\u001b[39;49maggregation)\n",
      "File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/tensorflow/python/ops/variable_scope.py:1275\u001b[0m, in \u001b[0;36mVariableScope.get_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m   1273\u001b[0m \u001b[39mif\u001b[39;00m dtype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1274\u001b[0m   dtype \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dtype\n\u001b[0;32m-> 1275\u001b[0m \u001b[39mreturn\u001b[39;00m var_store\u001b[39m.\u001b[39;49mget_variable(\n\u001b[1;32m   1276\u001b[0m     full_name,\n\u001b[1;32m   1277\u001b[0m     shape\u001b[39m=\u001b[39;49mshape,\n\u001b[1;32m   1278\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m   1279\u001b[0m     initializer\u001b[39m=\u001b[39;49minitializer,\n\u001b[1;32m   1280\u001b[0m     regularizer\u001b[39m=\u001b[39;49mregularizer,\n\u001b[1;32m   1281\u001b[0m     reuse\u001b[39m=\u001b[39;49mreuse,\n\u001b[1;32m   1282\u001b[0m     trainable\u001b[39m=\u001b[39;49mtrainable,\n\u001b[1;32m   1283\u001b[0m     collections\u001b[39m=\u001b[39;49mcollections,\n\u001b[1;32m   1284\u001b[0m     caching_device\u001b[39m=\u001b[39;49mcaching_device,\n\u001b[1;32m   1285\u001b[0m     partitioner\u001b[39m=\u001b[39;49mpartitioner,\n\u001b[1;32m   1286\u001b[0m     validate_shape\u001b[39m=\u001b[39;49mvalidate_shape,\n\u001b[1;32m   1287\u001b[0m     use_resource\u001b[39m=\u001b[39;49muse_resource,\n\u001b[1;32m   1288\u001b[0m     custom_getter\u001b[39m=\u001b[39;49mcustom_getter,\n\u001b[1;32m   1289\u001b[0m     constraint\u001b[39m=\u001b[39;49mconstraint,\n\u001b[1;32m   1290\u001b[0m     synchronization\u001b[39m=\u001b[39;49msynchronization,\n\u001b[1;32m   1291\u001b[0m     aggregation\u001b[39m=\u001b[39;49maggregation)\n",
      "File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/tensorflow/python/ops/variable_scope.py:520\u001b[0m, in \u001b[0;36m_VariableStore.get_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    518\u001b[0m   \u001b[39mreturn\u001b[39;00m custom_getter(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcustom_getter_kwargs)\n\u001b[1;32m    519\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 520\u001b[0m   \u001b[39mreturn\u001b[39;00m _true_getter(\n\u001b[1;32m    521\u001b[0m       name,\n\u001b[1;32m    522\u001b[0m       shape\u001b[39m=\u001b[39;49mshape,\n\u001b[1;32m    523\u001b[0m       dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m    524\u001b[0m       initializer\u001b[39m=\u001b[39;49minitializer,\n\u001b[1;32m    525\u001b[0m       regularizer\u001b[39m=\u001b[39;49mregularizer,\n\u001b[1;32m    526\u001b[0m       reuse\u001b[39m=\u001b[39;49mreuse,\n\u001b[1;32m    527\u001b[0m       trainable\u001b[39m=\u001b[39;49mtrainable,\n\u001b[1;32m    528\u001b[0m       collections\u001b[39m=\u001b[39;49mcollections,\n\u001b[1;32m    529\u001b[0m       caching_device\u001b[39m=\u001b[39;49mcaching_device,\n\u001b[1;32m    530\u001b[0m       partitioner\u001b[39m=\u001b[39;49mpartitioner,\n\u001b[1;32m    531\u001b[0m       validate_shape\u001b[39m=\u001b[39;49mvalidate_shape,\n\u001b[1;32m    532\u001b[0m       use_resource\u001b[39m=\u001b[39;49muse_resource,\n\u001b[1;32m    533\u001b[0m       constraint\u001b[39m=\u001b[39;49mconstraint,\n\u001b[1;32m    534\u001b[0m       synchronization\u001b[39m=\u001b[39;49msynchronization,\n\u001b[1;32m    535\u001b[0m       aggregation\u001b[39m=\u001b[39;49maggregation)\n",
      "File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/tensorflow/python/ops/variable_scope.py:473\u001b[0m, in \u001b[0;36m_VariableStore.get_variable.<locals>._true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    467\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m/part_0\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_vars:\n\u001b[1;32m    468\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    469\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mNo partitioner was provided, but a partitioned version of the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    470\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mvariable was found: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m/part_0. Perhaps a variable of the same \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    471\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mname was already created with partitioning?\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m name)\n\u001b[0;32m--> 473\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_single_variable(\n\u001b[1;32m    474\u001b[0m     name\u001b[39m=\u001b[39;49mname,\n\u001b[1;32m    475\u001b[0m     shape\u001b[39m=\u001b[39;49mshape,\n\u001b[1;32m    476\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m    477\u001b[0m     initializer\u001b[39m=\u001b[39;49minitializer,\n\u001b[1;32m    478\u001b[0m     regularizer\u001b[39m=\u001b[39;49mregularizer,\n\u001b[1;32m    479\u001b[0m     reuse\u001b[39m=\u001b[39;49mreuse,\n\u001b[1;32m    480\u001b[0m     trainable\u001b[39m=\u001b[39;49mtrainable,\n\u001b[1;32m    481\u001b[0m     collections\u001b[39m=\u001b[39;49mcollections,\n\u001b[1;32m    482\u001b[0m     caching_device\u001b[39m=\u001b[39;49mcaching_device,\n\u001b[1;32m    483\u001b[0m     validate_shape\u001b[39m=\u001b[39;49mvalidate_shape,\n\u001b[1;32m    484\u001b[0m     use_resource\u001b[39m=\u001b[39;49muse_resource,\n\u001b[1;32m    485\u001b[0m     constraint\u001b[39m=\u001b[39;49mconstraint,\n\u001b[1;32m    486\u001b[0m     synchronization\u001b[39m=\u001b[39;49msynchronization,\n\u001b[1;32m    487\u001b[0m     aggregation\u001b[39m=\u001b[39;49maggregation)\n",
      "File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/tensorflow/python/ops/variable_scope.py:857\u001b[0m, in \u001b[0;36m_VariableStore._get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    855\u001b[0m \u001b[39m# The code below handles only the case of creating a new variable.\u001b[39;00m\n\u001b[1;32m    856\u001b[0m \u001b[39mif\u001b[39;00m reuse \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 857\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mVariable \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m does not exist, or was not created with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    858\u001b[0m                    \u001b[39m\"\u001b[39m\u001b[39mtf.get_variable(). Did you mean to set \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    859\u001b[0m                    \u001b[39m\"\u001b[39m\u001b[39mreuse=tf.AUTO_REUSE in VarScope?\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m name)\n\u001b[1;32m    861\u001b[0m \u001b[39m# Create the tensor to initialize the variable with default value.\u001b[39;00m\n\u001b[1;32m    862\u001b[0m \u001b[39mif\u001b[39;00m initializer \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: Variable model/wpe does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=tf.AUTO_REUSE in VarScope?"
     ]
    }
   ],
   "source": [
    "session = gpt2.start_tf_sess()\n",
    "gpt2.load_gpt2(session, model_name='124M', reuse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bob Pytel has two apples in 5 seconds with 4 fish in 5 seconds - 43 (4-6)\n",
      "\n",
      "Molly Tyler has one egg in 20 seconds with 2 eggs in 20 seconds - 22 (6-8)\n",
      "\n",
      "Cameron Geist has one egg in 20 seconds with 1 egg in 20 seconds - 12 (9-11)\n",
      "\n",
      "Summer's Baking season has ended. It's time to start making your own. It's time for the end of summer. See the 16 big mistakes that will make your life better.\n",
      "\n",
      "Step-by-step\n",
      "\n",
      "Step 1: Here's how to make your own cookies:\n",
      "\n",
      "1. Preheat oven to 350° F.\n",
      "\n",
      "2. Spread your butter pan on a baking sheet.\n",
      "\n",
      "3. Bake until soft, about 15 minutes.\n",
      "\n",
      "4. Remove from oven.\n",
      "\n",
      "5. Let cool for 5 minutes.\n",
      "\n",
      "6. Remove the pan from the oven.\n",
      "\n",
      "7. Remove the butter from oven.\n",
      "\n",
      "8. Let cool for 15 minutes.\n",
      "\n",
      "Step 2: Once the butter is totally melted, place in a small bowl.\n",
      "\n",
      "9. Remove from oven.\n",
      "\n",
      "10. Let set for about 15 minutes.\n",
      "\n",
      "11. Remove from oven.<|endoftext|>Korean music star Yoonjoo approaches and speaks to fans during a concert in Seoul on Saturday. (EPA/Kwon Yoonjoo)\n",
      "\n",
      "Korean music star Yoonjoo visited Seoul on Saturday, where she shared a video of herself covering a song in front of a crowd, according to a Korean news agency.\n",
      "\n",
      "Yoonjoo posted the video on Facebook, saying she wanted to receive \"love\" from fans, adding, \"I didn't just want to show love to them.\"\n",
      "\n",
      "\"I wanted to be loved by everyone,\" she said, before taking a selfie with fans. \"And I also want to play music that will be special to my fans.\"\n",
      "\n",
      "The song, \"Ness,\" was written and performed by Yoonjoo's father, who has a passing illness. The video was shared with more than 29,000 people, with many expressing their love for the singer and expressing their support for her.\n",
      "\n",
      "\"I hope that I will keep playing my song for my fans,\" Yoonjoo wrote in the song. \"I hope that everyone will be happy. I hope the fans will be happy. I hope that they keep me coming.\"\n",
      "\n",
      "But an official spokesperson for the Korean Winter Association told AFP that the singer was not only unable to express her love for the fans, but had \"shown a lack of confidence in her abilities and her talent.\"\n",
      "\n",
      "\"Yoonjoo has been hospitalized for a few days, and is in serious condition,\" the spokesperson said.\n",
      "\n",
      "The singer's condition is considered serious, according to the Seoul Metropolitan Police.\n",
      "\n",
      "Yoonjoo will be performing at a performance of the musical \"Ness\" on Saturday, November 8, at the Seoul Winter Center for Music and the Arts in Seoul, according to a video on her Facebook page.\n",
      "\n",
      "(h/t: The Associated Press)<|endoftext|>Bitcoin and Bitcoin Cash\n",
      "\n",
      "\"Bitcoin,\" as it's known, is a digital currency created by Satoshi Nakamoto and more recently, the creator of Bitcoin, Gavin Andresen. Today the two man-made crypto-currency are in a state of flux and will only get bigger.\n",
      "\n",
      "Bitcoin is the first currency in the world that accepts bitcoins or cash. The currency is not backed by a government, which means it is not officially pegged to any particular currency. The only way to use the currency is through the Blockchain, which is a digital ledger that provides the final hash of transactions.\n",
      "\n",
      "The cryptocurrency utilizes a unique hashing algorithm called \"mining,\" which is said to be based on the idea of a single block. The blockchain also allows users to own and control the software that makes up the software. It is the start of the new era of cryptocurrency.\n",
      "\n",
      "The first Bitcoin Cash is scheduled for release on July 18, 2017, and it will be preceded by the first bitcoin-based payment method known as Bitcoin Cash. We hope that this announcement will give people an idea of how Bitcoin will change the way we use our wallets and wallets. Let us know what you think of Bitcoin Cash on our forum, or on Twitter.\n",
      "\n",
      "Bitcoin Cash is one of the first cryptocurrencies that is available for use with the Ethereum blockchain. The bitcoin-based payment system now supports local and central banking and allows for business transactions without having to go through the intermediary of an ATM.\n",
      "\n",
      "The first Bitcoin Cash transaction is represented by a \"signature digest\" that can be used to transfer value between computers using the network. The document is stored on the blockchain, and a special key is assigned to the original coin. The digest is then sent to a different computer and the value is transferred back to the original coin. The address\n"
     ]
    }
   ],
   "source": [
    "gpt2.generate(\n",
    "    session, \n",
    "    model_name='124M',\n",
    "    prefix='Bob Pytel has two apples in 5 seconds with 4 fish'\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shakespeare GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "session2 = gpt2.start_tf_sess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1732547164.326979 1837776 mlir_graph_optimization_pass.cc:401] MLIR V1 optimization pass is not enabled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint models/124M/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/124M/model.ckpt\n",
      "Loading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 13.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset has 22902 tokens\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 | 22.91] loss=1.89 avg=1.89\n",
      "[2 | 44.53] loss=2.14 avg=2.01\n",
      "[3 | 63.06] loss=2.34 avg=2.12\n",
      "[4 | 80.86] loss=1.65 avg=2.00\n",
      "[5 | 99.58] loss=1.66 avg=1.93\n",
      "[6 | 119.07] loss=2.00 avg=1.95\n",
      "[7 | 141.07] loss=1.78 avg=1.92\n",
      "[8 | 159.26] loss=1.86 avg=1.91\n",
      "[9 | 175.27] loss=1.52 avg=1.87\n",
      "[10 | 190.22] loss=1.70 avg=1.85\n",
      "[11 | 205.25] loss=1.38 avg=1.81\n",
      "[12 | 220.66] loss=1.83 avg=1.81\n",
      "[13 | 235.91] loss=1.70 avg=1.80\n",
      "[14 | 250.82] loss=1.43 avg=1.77\n",
      "[15 | 265.67] loss=1.73 avg=1.77\n",
      "[16 | 280.60] loss=1.49 avg=1.75\n",
      "[17 | 295.40] loss=1.43 avg=1.73\n",
      "[18 | 310.84] loss=1.61 avg=1.72\n",
      "[19 | 326.05] loss=1.67 avg=1.72\n",
      "[20 | 341.48] loss=1.37 avg=1.70\n",
      "[21 | 358.62] loss=1.56 avg=1.69\n",
      "[22 | 375.26] loss=1.18 avg=1.67\n",
      "[23 | 390.93] loss=1.47 avg=1.66\n",
      "[24 | 410.39] loss=1.01 avg=1.63\n",
      "[25 | 429.26] loss=1.31 avg=1.61\n",
      "[26 | 447.06] loss=1.27 avg=1.60\n",
      "[27 | 465.03] loss=1.19 avg=1.58\n",
      "[28 | 482.07] loss=0.96 avg=1.55\n",
      "[29 | 499.15] loss=1.18 avg=1.54\n",
      "[30 | 516.93] loss=0.97 avg=1.52\n",
      "[31 | 535.27] loss=1.15 avg=1.50\n",
      "[32 | 551.93] loss=1.01 avg=1.49\n",
      "[33 | 569.50] loss=0.97 avg=1.47\n",
      "[34 | 586.89] loss=1.15 avg=1.46\n",
      "[35 | 603.10] loss=1.18 avg=1.45\n",
      "[36 | 619.30] loss=0.89 avg=1.43\n",
      "[37 | 635.17] loss=1.09 avg=1.42\n",
      "[38 | 654.53] loss=1.17 avg=1.41\n",
      "[39 | 673.32] loss=1.02 avg=1.40\n",
      "[40 | 692.43] loss=1.07 avg=1.39\n",
      "[41 | 711.40] loss=1.02 avg=1.38\n",
      "[42 | 728.70] loss=0.91 avg=1.36\n",
      "[43 | 746.60] loss=0.66 avg=1.34\n",
      "[44 | 763.70] loss=0.78 avg=1.33\n",
      "[45 | 779.99] loss=0.79 avg=1.31\n",
      "[46 | 795.95] loss=0.73 avg=1.30\n",
      "[47 | 811.83] loss=0.76 avg=1.28\n",
      "[48 | 833.99] loss=0.95 avg=1.27\n",
      "[49 | 853.34] loss=0.63 avg=1.26\n",
      "[50 | 871.51] loss=0.73 avg=1.24\n",
      "[51 | 891.31] loss=0.68 avg=1.23\n",
      "[52 | 910.23] loss=0.52 avg=1.21\n",
      "[53 | 929.41] loss=0.61 avg=1.20\n",
      "[54 | 948.76] loss=0.74 avg=1.19\n",
      "[55 | 967.81] loss=0.58 avg=1.17\n",
      "[56 | 984.85] loss=0.50 avg=1.16\n",
      "[57 | 1003.10] loss=0.54 avg=1.14\n",
      "[58 | 1022.26] loss=0.63 avg=1.13\n",
      "[59 | 1040.55] loss=0.46 avg=1.12\n",
      "[60 | 1059.35] loss=0.47 avg=1.10\n",
      "[61 | 1078.17] loss=0.48 avg=1.09\n",
      "[62 | 1095.99] loss=0.59 avg=1.08\n",
      "[63 | 1114.23] loss=0.37 avg=1.06\n",
      "[64 | 1132.88] loss=0.56 avg=1.05\n",
      "[65 | 1151.81] loss=0.45 avg=1.04\n",
      "[66 | 1169.91] loss=0.36 avg=1.03\n",
      "[67 | 1187.83] loss=0.30 avg=1.01\n",
      "[68 | 1205.05] loss=0.39 avg=1.00\n",
      "[69 | 1223.23] loss=0.42 avg=0.99\n",
      "[70 | 1241.81] loss=0.39 avg=0.97\n",
      "[71 | 1259.54] loss=0.34 avg=0.96\n",
      "[72 | 1277.11] loss=0.29 avg=0.95\n",
      "[73 | 1295.27] loss=0.35 avg=0.94\n",
      "[74 | 1313.21] loss=0.29 avg=0.93\n",
      "[75 | 1332.06] loss=0.27 avg=0.91\n",
      "[76 | 1350.92] loss=0.37 avg=0.90\n",
      "[77 | 1371.29] loss=0.27 avg=0.89\n",
      "[78 | 1390.70] loss=0.27 avg=0.88\n",
      "[79 | 1408.14] loss=0.27 avg=0.87\n",
      "[80 | 1426.56] loss=0.24 avg=0.86\n",
      "[81 | 1444.33] loss=0.18 avg=0.85\n",
      "[82 | 1462.41] loss=0.22 avg=0.83\n",
      "[83 | 1481.53] loss=0.21 avg=0.82\n",
      "[84 | 1500.96] loss=0.23 avg=0.81\n",
      "[85 | 1521.54] loss=0.19 avg=0.80\n",
      "[86 | 1540.92] loss=0.17 avg=0.79\n",
      "[87 | 1560.29] loss=0.18 avg=0.78\n",
      "[88 | 1579.34] loss=0.15 avg=0.77\n",
      "[89 | 1599.51] loss=0.11 avg=0.76\n",
      "[90 | 1618.35] loss=0.15 avg=0.75\n",
      "[91 | 1637.59] loss=0.19 avg=0.74\n",
      "[92 | 1655.17] loss=0.17 avg=0.73\n",
      "[93 | 1676.47] loss=0.19 avg=0.72\n",
      "[94 | 1696.30] loss=0.15 avg=0.71\n",
      "[95 | 1716.85] loss=0.15 avg=0.70\n",
      "[96 | 1738.43] loss=0.14 avg=0.69\n",
      "[97 | 1759.06] loss=0.13 avg=0.68\n",
      "[98 | 1778.87] loss=0.15 avg=0.68\n",
      "[99 | 1799.07] loss=0.16 avg=0.67\n",
      "[100 | 1820.16] loss=0.09 avg=0.66\n",
      "Saving checkpoint/shakespeare/model-100\n"
     ]
    }
   ],
   "source": [
    "gpt2.finetune(\n",
    "    session2,\n",
    "    'shakespeare.txt',\n",
    "    model_name='124M',\n",
    "    steps=100,\n",
    "    run_name='shakespeare'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arion has a dog with the might of the power of the sword of lighting,\" she continues. \"This is so that the reader may notice the difference between the two.\"\n",
      "\n",
      "This is a developing story. Please refresh the page and continue reading.\n",
      "\n",
      "Correction: A previous version of this story incorrectly linked PETA with the Anti-Defamation League. PETA has also been linked to a non-profit called the \"Red Cross of Palestine.\"<|endoftext|>This is the third time in a row that a French politician has been found to have used a personal email account for official business.\n",
      "\n",
      "Adele Schlesinger, the former head of the State Department, used a personal account for official business. (Associated Press)\n",
      "\n",
      "The third time, the State Department has been linked to a secret and highly sensitive document, State Department records show.\n",
      "\n",
      "The State Department did not return a request for comment.\n",
      "\n",
      "The State Department did not return a request for comment.\n",
      "\n",
      "The State Department did not return a request for comment.\n",
      "\n",
      "The State Department did not return a request for comment.\n",
      "\n",
      "The State Department did not return a request for comment.\n",
      "\n",
      "The State Department did not return a request for comment.\n",
      "\n",
      "The State Department did not return a request for comment.\n",
      "\n",
      "The State Department did not return a request for comment.\n",
      "\n",
      "The State Department did not return a request for comment.\n",
      "\n",
      "The State Department did not return a request for comment.\n",
      "\n",
      "The State Department did not return a request for comment.\n",
      "\n",
      "The State Department did not return a request for comment.\n",
      "\n",
      "The State Department did not return a request for comment.\n",
      "\n",
      "The State Department did not return a request for comment.\n",
      "\n",
      "The State Department did not return a request for comment.\n",
      "\n",
      "The State Department did not return a request for comment.\n",
      "\n",
      "The State Department did not return a request for comment.\n",
      "\n",
      "The State Department did not return a request for comment.\n",
      "\n",
      "The State Department did not return a request for comment.\n",
      "\n",
      "The State Department did not return a request for comment.\n",
      "\n",
      "The State did not return a request for comment.\n",
      "\n",
      "The State did not return a request for comment.\n",
      "\n",
      "The State did not return a request for comment.\n",
      "\n",
      "The State did not return a request for comment.\n",
      "\n",
      "The State did not return a request for comment.\n",
      "\n",
      "The State did not return a request for comment.\n",
      "\n",
      "The State did not return a request for comment.\n",
      "\n",
      "The State did not return a request for comment.\n",
      "\n",
      "The State did not return a request for comment.\n",
      "\n",
      "The State did not return a request for comment.\n",
      "\n",
      "The State did not return a request for comment.\n",
      "\n",
      "The State did not return a request for comment.\n",
      "\n",
      "The State did not return a comment.\n",
      "\n",
      "The State did not return a request for comment.\n",
      "\n",
      "The State did not return a request for comment.\n",
      "\n",
      "The State did not return a comment.\n",
      "\n",
      "The State did not return a request for comment.\n",
      "\n",
      "The State did not return a request for comment.\n",
      "\n",
      "The State did not return a request for comment.\n",
      "\n",
      "The State did not return a request for comment.\n",
      "\n",
      "The State did not return a request for comment.\n",
      "\n",
      "The State did not return a request for comment.\n",
      "\n",
      "The State did not return a recommendation for reconsideration.\n",
      "\n",
      "The State did not return a recommendation for further review.\n",
      "\n",
      "The State did not return a recommendation for further review.\n",
      "\n",
      "The State did not return a recommendation for further review.\n",
      "\n",
      "The State did not return a recommendation for further review.\n",
      "\n",
      "The State did not return a recommendation for further review.\n",
      "\n",
      "The State did not return a recommendation for further review.\n",
      "\n",
      "The State did not return a recommendation for further review.\n",
      "\n",
      "The State did not return a recommendation for further review.\n",
      "\n",
      "The State did not return a recommendation for further review.\n",
      "\n",
      "The State did not return a recommendation for further review.\n",
      "\n",
      "The State did not return a request for comment.\n",
      "\n",
      "The State did not return a request for comment.\n",
      "\n",
      "The State did not return a request for comment.\n",
      "\n",
      "The State did not return a request for comment.\n",
      "\n",
      "The State did not return a request for comment.\n",
      "\n",
      "The State did not return a request for comment.\n",
      "\n",
      "The State did not return a request for comment.\n",
      "\n",
      "The State did not return a request for comment.\n",
      "\n",
      "The State did not return a request for comment.\n",
      "\n",
      "The State did not return a request for comment.\n",
      "\n",
      "The State did not return a request for comment.\n",
      "\n",
      "The State did not return a request for comment.\n",
      "\n",
      "The State did not return a request for comment.\n",
      "\n",
      "The State did not return a request for comment.\n",
      "\n",
      "The State did not return a request for comment.\n",
      "\n",
      "The State did not return a\n"
     ]
    }
   ],
   "source": [
    "gpt2.generate(\n",
    "    session2,\n",
    "    prefix='Arion has a dog with the might of the power of the sword of lighting',\n",
    "    run_name='shakespeare'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
